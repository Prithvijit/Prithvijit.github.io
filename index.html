<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- DELETE THIS SCRIPT if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');
  </script>

  <title>Prithvijit Chattopadhyay</title>

  <meta name="author" content="Prithvijit Chattopadhyay">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Prithvijit Chattopadhyay</name>
                  </p>
                  <p>
                    <!-- <strong><font color="blue">I'll be starting as a computer science PhD student at Georgia Tech in Fall 2019.</font></strong>  -->
                    I am a 3<sup>rd</sup> year CS PhD
                    student at Georgia Tech, advised by <a href="https://www.cc.gatech.edu/~judy/">Prof. Judy
                      Hoffman</a>.
                    <!-- I also collaborate with <a href="https://filebox.ece.vt.edu/~parikh/">Prof. Devi -->
                    <!-- Parikh</a> and <a href="https://filebox.ece.vt.edu/~dbatra/">Prof. Dhruv Batra</a>. -->
                    I earned
                    my Masters in Computer Science (with a focus on Machine Learning) in Spring 2019 from Georgia Tech,
                    advised
                    by <a href="https://filebox.ece.vt.edu/~parikh/">Prof. Devi Parikh</a>.
                    <!-- I also work closely with <a href="https://filebox.ece.vt.edu/~dbatra/">Prof. Dhruv Batra</a>. -->
                    Prior to joining Georgia Tech, I was working as a Research Assistant in the Computer Vision Machine
                    Learning and Perception Lab (CVMLP) at Virginia Tech, advised by <a
                      href="https://filebox.ece.vt.edu/~parikh/">Prof. Devi Parikh</a> and <a
                      href="https://filebox.ece.vt.edu/~dbatra/">Prof. Dhruv Batra</a>. I earned my Bachelors in
                    Electrical Engineering in 2016 from Delhi Technological University, India.
                  </p>

                  <p class="content">In the past couple of years, I have had the fortune to intern / conduct research at
                    <a href="https://prior.allenai.org/">PRIOR, Allen Institute of Artificial Intelligence</a> (Summer
                    2020) mentored by
                    <a href="https://anikem.github.io/">Ani Kembhavi</a> & <a href="https://roozbehm.info/">Roozbeh
                      Mottaghi</a>;
                    <a href="https://www.microsoft.com/en-us/research/group/deep-learning-group/">Deep Learning Group,
                      Microsoft Research Redmond</a> (Summer 2018) mentored by <a
                      href="https://www.microsoft.com/en-us/research/people/hpalangi/">Hamid Palangi</a>; <a
                      href="http://robotics.iiit.ac.in/">Robotics Research Lab, IIIT Hyderabad</a> (Winter 2014)
                    mentored by <a href="http://faculty.iiit.ac.in/~mkrishna/">Dr. K. Madhava Krishna</a> and <a
                      href="http://www.iacs.res.in/">Indian Association for the Cultivation of Science (IACS),
                      Kolkata</a> (Summer 2014) mentored by <a href="http://mailweb.iacs.res.in/theoph/tpssg/">Dr.
                      Soumitra Sengupta</a> on a diverse set of topics - ranging from embodied AI to vision & language
                    <!-- to
                    robotics  -->
                    to
                    theoretical physics.
                  </p>

                  <!-- <p class="content">Previously, I have worked with <a href="http://faculty.iiit.ac.in/~mkrishna/">Dr. K. Madhava Krishna</a> on problems relating to Robot Vision and Navigation at Robotics Research Lab, IIIT Hyderabad. My interest in robotics stemmed from being a part of Autonomous Underwater Vehicle Team in my undergrad days. I specifically worked on Underwater Acoustics and Control System problems relating to the bot.</p>

              <p class="content">I also find Theoretical Physics to be fascinating. I’ve had the privilege to work with <a href="http://mailweb.iacs.res.in/theoph/tpssg/">Dr. Soumitra Sengupta</a> on f(R) gravity. Our quasi-static journey towards a unified theory, joining Quantum Mechanics and General Relativity, is definitely a rewarding one. My interest in this specific branch of physics originated from trying to study Differential Geometry in my early undergrad days.</p>  -->

                  <!-- <p class="content">I occasionally play the percussion instrument Tabla. I am very passionate about
                    movies. I love to break them down shot-by-shot and analyze them. Every single frame is important.
                  </p> -->

                  <p style="text-align:center">
                    <a href="mailto:prithvijit3@gatech.edu">Email</a> &nbsp/&nbsp
                    <a href="data/CV.pdf">CV</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=rIK7AMkAAAAJ&hl=en">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/prithvijit-chattopadhyay-260b2b54/"> LinkedIn </a> &nbsp/&nbsp
                    <a href="https://github.com/prithv1"> Github </a> &nbsp/&nbsp
                    <a href="https://twitter.com/prithvijitch"> Twitter </a> &nbsp/&nbsp
                    <a href="https://www.instagram.com/prithv12/?hl=en"> Instagram </a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/prithv1.png"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/prithv1-circ.png" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research Interests</heading>
                  <p>
                    <!-- I am broadly interested in problems at the intersection of Computer Vision and Natural Language Processing. -->
                    <!-- The problems that I work on generally lie at the intersection of Computer Vision and Natural
                    Language Processing.
                    More specifically,  -->
                    <!-- I am interested in developing
                    reliable computer vision systems
                    <!-- intelligent systems  -->
                    <!-- (including applications of
                    Machine Learning and
                    Reinforcement Learning) that -->
                    <!-- The problems that I work on lie at the intersection of Computer Vision, Machine Learning and Natural Language Processing. Specifically, I am interested in developing AI systems that -->
                    I'm broadly interested in problems at the intersection of Computer Vision, Reinforcement Learning
                    and Machine Learning. More specifically,
                    I'm interested in the problem of out-of-distribution generalization - how can we develop
                    systems (reliant on vision as a modality) that can be adapted
                    across novel scenarios with ease and limited supervision.

                  </p>
                  <!-- <p class="content">
                  <ul>
                    <li>can <b><i>perceive and reason</i></b> based on multimodal sensory information</li>
                    <li>are <b><i>interpretable</i></b> so that predictions made by such systems can be explained</li>
                    <li>are <b><i>transferable</i></b> so that they can be adapted across different domains with ease
                      and limited supervision</li>
                  </ul>
                  </p> -->
                  <p>
                    Representative papers are <span class="highlight">listed under Papers</span>.
                  </p>

                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td>
                <heading>Achievements</heading>
                <ul>
                  <li>Recognized as an <a href="http://cvpr2021.thecvf.com/node/184">outstanding reviewer
                    </a> for CVPR 2021!</li>
                  <li>Recognized to be among the <a href="data/Prithvijit Chattopadhyay_ICML_Certificate.pdf">top 33
                      percent reviewers</a> for ICML 2020!</li>
                  <li><a href="https://arxiv.org/abs/2008.11300">Likelihood Landscapes</a> received the NVIDIA Best
                    Runner Up paper award at AROW, ECCV 2020!</li>
                  <li>Awarded the College of Computing's <a
                      href="https://www.cc.gatech.edu/annual-awards-and-honors-past-recipients">College of Computing's Rising Star Doctoral Student Research Award (formerly known as the CS7001 Research
                      Award)</a> at Georgia Tech!</li>
                  <li>Recognized as one of the highest-scoring reviewers for NeurIPS 2019!</li>
                  <li>Recognized as an <a href="https://iclr.cc/Conferences/2019/Awards">outstanding reviewer</a> for
                    ICLR 2019!</li>
                  <li>Recognized to be among the top 30 percent highest scoring reviewers for NeurIPS 2018!</li>
                  <li>Awarded the College of Computing's MS Research Award at Georgia Tech!</li>
                  <li>Our team won <a href="http://www.vthacks.com/">VT Hacks 2017</a>, a <a href="https://mlh.io/">
                      Major League Hacking Event</a>, 2017!</li>
                  <li>Our undergraduate team, DTU-AUV, qualified for the semi-finals at <a
                      href="http://www.auvsifoundation.org/2014-robosub-teams">AUVSI Robosub 2014</a>!</li>
                  <li>Awarded Merit Scholarships from 2012-2014 for undergraduate academic performance!</li>
                  <li>Selected for <a href="http://kvpy.iisc.ernet.in/main/index.htm">KVPY</a> and <a
                      href="http://www.inspire-dst.gov.in/fellowship.html">INSPIRE</a> Fellowships, 2012 for
                    undergraduate studies in basic sciences!</li>
                  <li>Placed among the top 1 percent students in the country in <a
                      href="http://www.indapt.org/index.php/inphoipho">INPhO</a> 2012!</li>
                  <li>Selected for rigorous mathematical training camps conducted by mathematicians from Bhabha Atomic
                    Research Center (<a href="http://www.barc.gov.in/">BARC</a>) and Indian Institute of Science (<a
                      href="http://www.iisc.ac.in/">IISc</a>) in 2012!</li>
                  <li>Selected for <a href="http://www.csirhrdg.res.in/cpyls.htm">CSIR Programme on Youth Leadership in
                      Science</a>, 2010!</li>

                </ul>
              </td>
            </tr>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td>
                <heading>News</heading>
                <ul>
                  <li>May 2020: I'll be interning in the <a href="https://prior.allenai.org/">PRIOR Team</a> at Allen
                    Institute of Artificial Intelligence.</li>
                  <li> Serving as a reviewer for <a href="http://cvpr2018.thecvf.com/">CVPR 2018-22</a>,
                    <a href="https://www.icra2022.org/">ICRA 2021-22</a>,
                    <a href="https://eccv2018.org/">ECCV 2018</a>, <a href="https://nips.cc/">NeurIPS 2018-21</a>, <a
                      href="https://iclr.cc/">ICLR 2019-22</a>, <a href="https://icml.cc/Conferences/2020">ICML
                      2019-20</a>, <a href="http://www.acl2019.org/EN/index.xhtml">ACL 2019</a>.
                  </li>
                  <li>May 2019: Presented our work on Discovery of Decision States through Intrinsic Control at the <a
                      href="https://tarl2019.github.io">Task-Agnostic Reinforcement Learning (TARL) Workshop</a> at ICLR
                    2019.</li>
                  <li>May 2019:<font color="red"><strong> Completed my Masters in Computer Science (focus on Machine
                        Learning)</font> </strong> with a thesis centered on <a
                      href="https://smartech.gatech.edu/handle/1853/61308">Evaluating Visual Conversational Agents in
                      the Context of Human-AI Cooperative Games</a>!</li>
                  <li>Feb 2019: Our technical report describing <a href="http://evalai.cloudcv.org/">EvalAI</a> - an
                    open source platform to evaluate and compare AI algorithms at scale - is out on <a
                      href="https://arxiv.org/pdf/1902.03570.pdf"> ArXiv</a>!</li>
                  <li>Dec 2018: Presented our work <a href="https://arxiv.org/abs/1808.02861">interpretable zero-shot
                      learning</a> at the <a
                      href="https://sites.google.com/view/continual2018/home?authuser=0">Continual Learning </a> and <a
                      href="https://nips2018vigil.github.io/">Visually Grounded Interaction and Language (ViGIL)</a>
                    workshops at NeurIPS 2018.</li>
                  <li>Aug 2018: Our paper titled <a href="https://arxiv.org/pdf/1810.12366.pdf">'Do explanation
                      modalities make VQA models more predictable to a human?'</a></a> was accepted in <a
                      href="http://emnlp2018.org/">EMNLP, 2018</a>!</li>
                  <li>Jul 2018: Our paper titled <a href="https://arxiv.org/abs/1808.02861">'Choose Your Neuron:
                      Incorporating Domain Knowledge through Neuron-Importance'</a> was accepted in <a
                      href="https://eccv2018.org/">ECCV, 2018</a>!</li>
                  <li>Apr 2018: I was awarded the College of Computing's MS Research Award at Georgia Tech!</li>
                  <li>Feb 2018: I'll intern in the Deep Learning Group at <a
                      href="https://www.microsoft.com/en-us/research/group/deep-learning-group/">Microsoft Research,
                      Redmond</a> in summer 2018.</li>
                  <li>Aug 2017: Our paper titled <a href="https://arxiv.org/abs/1708.05122">'Evaluating Visual
                      Conversational Agents via Cooperative Human-AI Games'</a> was accepted in <a
                      href="https://www.humancomputation.com/2017/">HCOMP 2017</a> as an oral!</li>
                  <li>Jul 2017: I will be joining Georgia Tech as a Masters of Computer Science student in Fall 2017.
                  </li>
                  <li>May 2017: I will be presenting <a href="https://arxiv.org/abs/1604.03505">'Counting Everyday
                      Objects in Everyday Scenes'</a> at the <a href="http://www.ldv.co/visionsummit/">LDV Vision
                      Summit, 2017</a>.</li>
                  <li>Mar 2017: Our paper titled <a href="https://arxiv.org/abs/1704.00717">'It Takes Two to Tango:
                      Towards Theory of AI's Mind'</a> is out on ArXiv!</li>
                  <li>Feb 2017: Our paper titled <a href="https://arxiv.org/abs/1604.03505">'Counting Everyday Objects
                      in Everyday Scenes'</a> was accepted in <a href="http://cvpr2017.thecvf.com/"> CVPR 2017 </a> as a
                    spotlight!</li>
                  <li>Feb 2017: Our team built <a href="https://devpost.com/software/filterai">FilterAI</a> - an image
                    retrival engine - and won <a href="http://www.vthacks.com/">VT Hacks 2017</a>, a <a
                      href="https://mlh.io/"> Major League Hacking Event</a>!</li>
                  <li>Dec 2016: <a href="https://arxiv.org/abs/1604.03505">'Counting Everyday Objects in Everyday
                      Scenes'</a></a> received an <a
                      href="https://aws.amazon.com/blogs/publicsector/call-for-computer-vision-research-proposals-with-new-amazon-bin-image-data-set/">
                      Amazon Academic Research Award</a>, 2016!</li>
                </ul>
              </td>
            </tr>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading>Papers</heading> (<sup>*</sup> joint first authors)
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <!-- <heading>Publications</heading> -->
              <tr>
                <td width="30%"><img src="images/robustnav.png" alt="3DSP" width="230" height="150"
                    style="border-style: none">
                <td valign="top" width="70%">
                  <a href="https://arxiv.org/abs/2106.04531">
                    <papertitle>RobustNav: Towards Benchmarking Robustness in Embodied Navigation
                    </papertitle>
                  </a>
                  <br>
                  <strong><u>Prithvijit Chattopadhyay</u></strong>,
                  Judy Hoffman,
                  Roozbeh Mottaghi,
                  Ani Kembhavi
                  <br>
                  <em>ICCV</em>, 2021 <font color="red">(Oral Presentation)</font><br>
                  <a href="https://arxiv.org/abs/2106.04531">arxiv</a> /
                  <a href="https://github.com/allenai/robustnav">code</a> /
                  <a href="https://prior.allenai.org/projects/robustnav">project page</a> /
                  <a href="https://www.youtube.com/watch?v=wqVGsuUbm48">video</a>
                  <p align="justify">As an attempt towards assessing the robustness of embodied navigation agents, we
                    propose RobustNav, a framework to quantify the performance of embodied navigation agents when
                    exposed to a wide variety of visual – affecting RGB inputs – and dynamics – affecting transition
                    dynamics – corruptions. Most recent efforts in visual navigation have typically focused on
                    generalizing to novel target environments with similar appearance and dynamics characteristics. With
                    RobustNav, we find that some standard embodied navigation agents significantly underperform (or
                    fail) in the presence of visual or dynamics corruptions. We systematically analyze the kind of
                    idiosyncrasies that emerge in the behavior of such agents when operating under corruptions. Finally,
                    for visual corruptions in RobustNav, we show that while standard techniques to improve robustness
                    such as data-augmentation and self-supervised adaptation offer some zero-shot resistance and
                    improvements in navigation performance, there is still a long way to go in terms of recovering lost
                    performance relative to clean “non-corrupt” settings, warranting more research in this direction.
                  </p>
                </td>
              </tr>

              <tr>
                <td width="30%"><img src="images/likelihood_landscapes.png" alt="3DSP" width="250" height="150"
                    style="border-style: none">
                <td valign="top" width="70%">
                  <a href="https://arxiv.org/abs/2008.11300">
                    <papertitle>Likelihood Landscapes: A Unifying Principle Behind Many Adversarial Defenses
                    </papertitle>
                  </a>
                  <br>
                  Fu Lin,
                  Rohit Mittapali,
                  <strong><u>Prithvijit Chattopadhyay</u></strong>,
                  Daniel Bolya,
                  Judy Hoffman
                  <br>
                  <em>Adversarial Robustness in the Real World (AROW), ECCV</em> 2020</a>
                  <font color="red">(Poster)</font><br>
                  <em>
                    <font color="red">NVIDIA Best Paper Runner Up</font>
                  </em><br>
                  <a href="https://arxiv.org/abs/2008.11300">paper</a> /
                  <a href="https://www.youtube.com/watch?v=RuLY6UNIeOs">video</a>
                  <p align="justify">Convolutional Neural Networks (CNNs) have been shown to be vulnerable to
                    adversarial examples, which are known to locate in subspaces close to where normal data lies but are
                    not naturally occurring and have low probability. In this work, we investigate the potential effect
                    defense techniques have on the geometry of the likelihood landscape - likelihood of the input images
                    under the trained model. We first propose a way to visualize the likelihood landscape by leveraging
                    an energy-based model interpretation of discriminative classifiers. Then we introduce a measure to
                    quantify the flatness of the likelihood landscape. We observe that a subset of adversarial defense
                    techniques results in a similar effect of flattening the likelihood landscape. We further explore
                    directly regularizing towards a flat landscape for adversarial robustness.</p>
                </td>
              </tr>

              <tr>
                <td width="30%"><img src="images/DMG_approach.png" alt="3DSP" width="250" height="160"
                    style="border-style: none">
                <td valign="top" width="70%">
                  <a href="https://arxiv.org/abs/2008.12839">
                    <papertitle>Learning to Balance Specificity and Invariance for In and Out of Domain Generalization
                    </papertitle>
                  </a>
                  <br>
                  <strong><u>Prithvijit Chattopadhyay</u></strong>,
                  Yogesh Balaji,
                  Judy Hoffman
                  <br>
                  <em>ECCV</em>, 2020 <font color="red">(Poster)</font><br>
                  <a href="https://arxiv.org/abs/2008.12839">paper</a> /
                  <a href="https://www.youtube.com/watch?v=u4YQdV8dJWM">video</a> /
                  <a href="https://github.com/prithv1/DMG/tree/master">code</a>
                  <p align="justify">We introduce Domain-specific Masks for Generalization, a model for improving both
                    in-domain and out-of-domain generalization performance. For domain generalization, the goal is to
                    learn from a set of source domains to produce a single model that will best generalize to an unseen
                    target domain. As such, many prior approaches focus on learning representations which persist across
                    all source domains with the assumption that these domain agnostic representations will generalize
                    well. However, often individual domains contain characteristics which are unique and when leveraged
                    can significantly aid in-domain recognition performance. To produce a model which best generalizes
                    to both seen and unseen domains, we propose learning domain specific masks. The masks are encouraged
                    to learn a balance of domain-invariant and domain-specific features, thus enabling a model which can
                    benefit from the predictive power of specialized features while retaining the universal
                    applicability of domain-invariant features. We demonstrate competitive performance compared to naive
                    baselines and state-of-the-art methods on both PACS and DomainNet.</p>
                </td>
              </tr>

              <tr>
                <td width="30%"><img src="images/visdial_div_teaser.jpg" alt="3DSP" width="220" height="150"
                    style="border-style: none">
                <td valign="top" width="70%">
                  <a href="https://arxiv.org/abs/1909.10470">
                    <papertitle>Improving Generative Visual Dialog by Answering Diverse Questions
                    </papertitle>
                  </a>
                  <br>
                  Vishvak Murahari,
                  <strong><u>Prithvijit Chattopadhyay</u></strong>,
                  Dhruv Batra,
                  Devi Parikh,
                  Abhishek Das
                  <br>
                  <em>EMNLP</em>, 2019 <font color="red">(Poster)</font>; <a
                    href="https://visualqa.org/workshop.html"><em>Visual Question Answering and Dialog Workshop,
                      CVPR</em> 2019</a>
                  <font color="red">(Poster)</font>
                  <!-- <font color="red">(Poster)</font> -->
                  <br>
                  <a href="https://arxiv.org/abs/1909.10470">arxiv</a> /
                  <a href="https://github.com/vmurahari3/visdial-diversity">code</a>
                  <p align="justify">While generative visual dialog models trained with self-talk based RL perform
                    better at the associated downstream task, they suffer from repeated interactions -- resulting in
                    saturation in improvements as the number of rounds increase. To counter this, we devise a simple
                    auxiliary objective that incentivizes Q-Bot to ask diverse questions, thus reducing repetitions and
                    in turn enabling A-Bot to explore a larger state space during RL i.e., be exposed to more visual
                    concepts to talk about, and varied questions to answer.</p>
                </td>
              </tr>


              <tr>
                <td width="30%"><img src="images/ds_vic_new.png" alt="3DSP" width="230" height="200"
                    style="border-style: none">
                <td valign="top" width="70%">
                  <a href="https://arxiv.org/abs/1907.10580">
                    <papertitle>DS-VIC: Unsupervised Discovery of Decision States for Transfer in RL
                    </papertitle>
                  </a>
                  <br>
                  Nirbhay Modhe,
                  <strong><u>Prithvijit Chattopadhyay</u></strong>,
                  Mohit Sharma,
                  Abhishek Das,
                  Devi Parikh,
                  Dhruv Batra,
                  Ramakrishna Vedantam
                  <br>
                  <em>arxiv Preprint</em>, 2019; <a href="https://tarl2019.github.io/"><em>Task-Agnostic RL (TARL)
                      Workshop, ICLR</em> 2019</a>
                  <font color="red">(Poster)</font><br>
                  <a href="https://arxiv.org/abs/1907.10580">arxiv</a> /
                  <a href="https://tarl2019.github.io/assets/papers/modhe2019unsupervised.pdf">TARL'19 Preliminary
                    Version</a><br>
                  <a href="https://arxiv.org/abs/1907.10580">(Revised version accepted in IJCAI 2020!)</a>
                  <p align="justify">We learn to identify decision states, namely the parsimonious set of states where
                    decisions meaningfully affect the future states an agent can reach in an environment. We utilize the
                    VIC framework, which maximizes an agent's 'empowerment', i.e. the ability to reliably reach a
                    diverse set of states -- and formulate a sandwich bound on the empowerment objective that allows
                    identification of <i>decision states</i>. Unlike previous work, our decision states are discovered
                    without extrinsic rewards -- simply by interacting with the world. Our results show that our
                    decision states are: (1) often interpretable, and (2) lead to better exploration on downstream
                    goal-driven tasks in partially observable environments.</p>
                </td>
              </tr>

              <tr>
                <td width="30%"><img src="images/evalai_teaser.png" alt="3DSP" width="220" height="180"
                    style="border-style: none">
                <td valign="top" width="70%">
                  <a href="https://arxiv.org/pdf/1902.03570">
                    <papertitle>EvalAI: Towards Better Evaluation Systems for AI Agents</papertitle>
                  </a>
                  <br>
                  Deshraj Yadav,
                  Rishabh Jain,
                  Harsh Agrawal,
                  <strong><u>Prithvijit Chattopadhyay</u></strong>,
                  Taranjeet Singh,
                  Akash Jain,
                  Shiv Baran Singh,
                  Stefan Lee,
                  Dhruv Batra
                  <br>
                  <em>arxiv Preprint</em>, 2019<br>
                  <a href="https://arxiv.org/abs/1902.03570">arxiv</a> /
                  <a href="https://github.com/Cloud-CV/EvalAI">code</a>
                  <p align="justify">We introduce EvalAI, an open source platform for evaluating and comparing machine
                    learning (ML) and artificial intelligence algorithms (AI) at scale. EvalAI is built to provide a
                    scalable solution to the research community to fulfill the critical need of evaluating machine
                    learning models and agents acting in an environment against annotations or with a human-in-the-loop.
                    This will help researchers, students, and data scientists to create, collaborate, and participate in
                    AI challenges organized around the globe.</p>
                </td>
              </tr>

              <tr>
                <td width="30%"><img src="images/NIWT_teaser.png" alt="3DSP" width="240" height="200"
                    style="border-style: none">
                <td valign="top" width="70%">
                  <a href="https://arxiv.org/abs/1808.02861">
                    <papertitle>Choose Your Neuron: Incorporating Domain-Knowledge through Neuron-Importance
                    </papertitle>
                  </a>
                  <br>
                  Ramprasaath R. Selvaraju<sup>*</sup>,
                  <strong><u>Prithvijit Chattopadhyay</u></strong><sup>*</sup>,
                  Mohamed Elhoseiny,
                  Tilak Sharma,
                  Dhruv Batra,
                  Devi Parikh,
                  Stefan Lee
                  <br>
                  <em>ECCV</em>, 2018 <font color="red">(Poster)</font>; <a
                    href="https://sites.google.com/view/continual2018/home?authuser=0"><em>Continual Learning Workshop,
                      NeurIPS</em> 2018</a>
                  <font color="red">(Poster)</font>; <a href="https://nips2018vigil.github.io/"><em>Visually Grounded
                      Interaction and Language (ViGIL) Workshop, NeurIPS</em> 2018</a>
                  <font color="red">(Poster)</font><br>
                  <a href="https://arxiv.org/abs/1808.02861">arxiv</a> /
                  <a
                    href="https://mlatgt.blog/2018/09/05/choose-your-neuron-incorporating-domain-knowledge-through-neuron-importance/">blogpost</a>
                  /
                  <a href="https://github.com/ramprs/neuron-importance-zsl">code</a>
                  <p align="justify"> We introduce a simple, efficient zero-shot learning approach -- NIWT -- based on
                    the observation that individual neurons in CNNs have been shown to implicitly learn a dictionary of
                    semantically meaningful concepts (simple textures and shapes to whole or partial objects). NIWT
                    learns to map domain knowledge about "unseen" classes onto this dictionary of learned concepts and
                    optimizes for network parameters that can effectively combine these concepts - essentially learning
                    classifiers by discovering and composing learned semantic concepts in deep networks.</p>
                </td>
              </tr>

              <tr>
                <td width="30%"><img src="images/explanations_predictable.png" alt="3DSP" width="205" height="200"
                    style="border-style: none">
                <td valign="top" width="70%">
                  <a href="https://arxiv.org/abs/1810.12366">
                    <papertitle>Do explanation modalities make VQA Models more predictable to a human?</papertitle>
                  </a>
                  <br>
                  Arjun Chandrasekaran<sup>*</sup>,
                  Viraj Prabhu<sup>*</sup>,
                  Deshraj Yadav<sup>*</sup>,
                  <strong><u>Prithvijit Chattopadhyay</u></strong><sup>*</sup>,
                  Devi Parikh
                  <br>
                  <em>EMNLP</em>, 2018 <font color="red">(Poster)</font><br>
                  <a href="https://arxiv.org/pdf/1810.12366">arxiv</a>
                  <p align="justify">A rich line of research attempts to make deep neural networks more transparent by
                    generating human-interpretable 'explanations' of their decision process, especially for interactive
                    tasks like Visual Question Answering (VQA). In this work, we analyze if existing explanations indeed
                    make a VQA model -- its responses as well as failures -- more predictable to a human.</p>
                </td>
              </tr>

              <tr>
                <td width="30%"><img src="images/eval_visdial.png" alt="3DSP" width="180" height="220"
                    style="border-style: none">
                <td valign="top" width="70%">
                  <a href="https://arxiv.org/abs/1708.05122">
                    <papertitle>Evaluating Visual Conversational Agents via Cooperative Human-AI Games</papertitle>
                  </a>
                  <br>
                  <strong><u>Prithvijit Chattopadhyay</u></strong><sup>*</sup>,
                  Deshraj Yadav<sup>*</sup>,
                  Viraj Prabhu,
                  Arjun Chandrasekaran,
                  Abhishek Das,
                  Stefan Lee,
                  Dhruv Batra,
                  Devi Parikh
                  <br>
                  <em>HCOMP</em>, 2017 <font color="red">(Oral)</font><br>
                  <a href="https://arxiv.org/abs/1708.05122">arxiv</a> /
                  <a href="https://github.com/GT-Vision-Lab/GuessWhich">code</a>
                  <p align="justify">We design a cooperative game - GuessWhich - to measure human-AI team performance in
                    the specific context of the AI being a visual conversational agent. GuessWhich involves live
                    interaction between the human and the AI and is designed to gauge the extent to which progress in
                    isolated metrics for AI (& AI-AI teams) transfers to human-AI collaborative scenarios.</p>
                </td>
              </tr>

              <tr>
                <td width="30%"><img src="images/toaim.png" alt="3DSP" width="200" height="120"
                    style="border-style: none">
                <td valign="top" width="70%">
                  <a href="https://arxiv.org/abs/1704.00717">
                    <papertitle>It Takes Two to Tango: Towards Theory of AI's Mind</papertitle>
                  </a>
                  <br>
                  Arjun Chandrasekaranu<sup>*</sup>,
                  Deshraj Yadav<sup>*</sup>,
                  <strong><u>Prithvijit Chattopadhyay</u></strong><sup>*</sup>,
                  Viraj Prabhu<sup>*</sup>,
                  Devi Parikh
                  <br>
                  <em>Chalearn Looking at People Workshop, CVPR</em>, 2017 <font color="red">(Oral)</font><br>
                  <a href="https://arxiv.org/abs/1704.00717">arxiv</a> /
                  <a href="https://github.com/deshraj/TOAIM">code</a>
                  <p align="justify">To effectively leverage the progress in Artificial Intelligence (AI) to make our
                    lives more productive, it is important for humans and AI to work well together in a team. In this
                    work, we argue that for human-AI teams to be effective, in addition to making AI more accurate and
                    human-like, humans must also develop a theory of AI's mind (ToAIM) - get to know its strengths,
                    weaknesses, beliefs, and quirks. </p>
                </td>
              </tr>

              <tr>
                <td width="30%"><img src="images/counting_task_img.jpg" alt="3DSP" width="230" height="140"
                    style="border-style: none">
                <td valign="top" width="70%">
                  <a href="https://arxiv.org/abs/1604.03505">
                    <papertitle>Counting Everyday Objects in Everyday Scenes</papertitle>
                  </a>
                  <br>
                  <strong><u>Prithvijit Chattopadhyay</u></strong><sup>*</sup>,
                  Ramakrishna Vedantam<sup>*</sup>,
                  Ramprasaath R. Selvaraju,
                  Dhruv Batra,
                  Devi Parikh
                  <br>
                  <em>CVPR</em>, 2017 <font color="red">(Spotlight)</font><br>
                  <a href="https://arxiv.org/abs/1604.03505">arxiv</a> /
                  <a href="https://github.com/prithv1/cvpr2017_counting">code</a>
                  <p align="justify">We study the numerosity of object classes in natural, everyday images and build
                    dedicated models for counting designed to tackle the large variance in counts, appearances, and
                    scales of objects found in natural scenes. We propose a contextual counting approach inspired by the
                    phenomenon of subitizing - the ability of humans to make quick assessments of counts given a
                    perceptual signal, for small count values.</p>
                </td>
              </tr>

            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <hr>
                  <p align="center">
                    <font>(Design and CSS courtesy: <a href="https://jonbarron.info/">Jon Barron</a> and <a
                        href="https://abhoi.github.io/">Amlaan Bhoi</a>)</font>
                  </p>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>